### A Pluto.jl notebook ###
# v0.14.1

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# â•”â•â•¡ 400ebe26-0dea-4cf2-8744-6c73a45cd33e
using PlutoUI, Plots, Statistics, Optim, JuMP, Ipopt

# â•”â•â•¡ 945c2bf1-d7dc-42c9-93d7-fd754f8fb1d7
html"""
<div style="
position: absolute;
width: calc(100% - 30px);
border: 50vw solid #282936;
border-top: 500px solid #282936;
border-bottom: none;
box-sizing: content-box;
left: calc(-50vw + 15px);
top: -500px;
height: 500px;
pointer-events: none;
"></div>

<div style="
height: 500px;
width: 100%;
background: #282936;
color: #fff;
padding-top: 68px;
">
<span style="
font-family: Vollkorn, serif;
font-weight: 700;
font-feature-settings: 'lnum', 'pnum';
"> <p style="
font-size: 1.5rem;
opacity: .8;
"><em>Section 2.9 </em></p>
<p style="text-align: center; font-size: 2rem;">
<em> Optimization </em>
</p>


</div>
</div>

<style>
body {
overflow-x: hidden;
}
</style>"""

# â•”â•â•¡ b8d66df5-f593-40b4-8c46-3b638f9cc3e1
TableOfContents(title="ğŸ“š Table of Contents", aside=true)

# â•”â•â•¡ dccbd53d-33ed-4d37-9d2c-da76e090d5dd
md"""
# Line Fitting Many Ways
"""

# â•”â•â•¡ 2ed86f33-bced-413c-9a8d-c6e49bfe5afb
md"""
# Exploratory Data Analysis
"""

# â•”â•â•¡ 0e43a6d3-7198-422b-b50c-b9caeaa53074
md"""
n = $(@bind n Slider(3:10:200, show_value=true))
"""

# â•”â•â•¡ f8c98995-2152-4d45-996a-a0532a233719
x = sort((rand( -10:100, n)))

# â•”â•â•¡ 8a5f1fdc-3cef-4c02-a73f-e5975b57b15a
y = 5/9 .* x  .- 17.7777777  .+  5 .* randn.() #  same as y =  5/9 .* (x .- 32)

# â•”â•â•¡ 647093eb-a7e3-4175-8091-29c33407e5c9
begin	
	plot(x,y, m=:c, mc=:red,legend=false)
	xlabel!("Â°F")
	ylabel!("Â°C")
	# plot!( x, (x.-30)./2) Dave's cool approximation
end

# â•”â•â•¡ cdc25782-65a8-43c5-8090-c1241b798b1a
md"""
# Least Squares fitting to a straight line
"""

# â•”â•â•¡ 9ec4dd43-c95a-4f11-b844-fd6ccc93bb68
md"""
Suppose we are given data $x_i$ and measurements $y_i$, least squares fitting a straight line means finding the best `m` (slope) and `b` intercept that minimizes
the "error" in a least squares sense:

``\min_{m,b} \sum  ( (b + m x_i) - y_i)^2 `` 
"""

# â•”â•â•¡ 9276b315-27b2-4b01-8fc8-4ebbba58d080
md"""
# Direct Formulas
"""

# â•”â•â•¡ d22fd4bd-acfe-4e27-a484-3c2d6138f44e
md"""
## The Statistician's formula
"""

# â•”â•â•¡ da0d208b-7d30-470a-b180-4cbfa98298e7
begin
	m = cov(x,y)/var(x) # same as (x.-mean(x))â‹…(y.-mean(y))/sum(abs2,x.-mean(x))
	b = mean(y) - m * mean(x)
	(b=b, m=m)
end

# â•”â•â•¡ 6cf233a7-9b8b-47aa-a3ad-2440d001af73
md"""
### Julia: Named Tuples
"""

# â•”â•â•¡ 613c3e5f-bbdd-4cf9-b30f-69e2c42ae0ec
nt = (first=1, next=2, last=3.1) # kind of handy

# â•”â•â•¡ 4cce580b-0032-419c-b386-e470b084ab96
typeof( nt )

# â•”â•â•¡ 5503b4de-0b53-4223-8ce0-5e014be3f7ab
plot!( x-> m*x+b, lw=4 )

# â•”â•â•¡ 05e512ca-3123-48d9-9c11-5d6e9d90ef95
md"""
## The Linear Algebraist's Formula
"""

# â•”â•â•¡ e0b4c2a9-a68b-47af-bf9c-f1a9f0256fd4
[one.(x) x]\y  # even shorter but you need to know linear algebra, but generalizes

# â•”â•â•¡ 6d25e38e-c18a-48b3-8b12-b670f5a5180f
md"""
# Optimization Methods

Since the problem is an optimization problem, we can use optimization software to obtain an answer.  This is overkill for lines, but generalizes to so many nonlinear situations including neural networks as in machine learning.
"""

# â•”â•â•¡ f291c0cb-51ee-4b30-9e07-e7cf374f809e
md"""
## Optim.jl: A package written entirely in Julia for optimization
"""

# â•”â•â•¡ aa06a447-d6c5-48ee-9864-c1f431fe5e4b
md"""
[Optim.jl Documentation](https://julianlsolvers.github.io/Optim.jl/stable/#)
"""

# â•”â•â•¡ d3edfb26-7258-45a3-a88c-60831338df1f
md"""
We can  ask software to just solve the problem: ``\min_{b,m} \sum_{i=1}^n  ( (b + m x_i) - y_i)^2 `` 
"""

# â•”â•â•¡ 372b304a-3f57-4bec-88df-3d51ded57d5c
loss((b,m)) = sum(  (b + m*x[i] - y[i])^2  for i=1:n)

# â•”â•â•¡ 13b9ff38-225d-4ec1-be7f-bf0e0f5b4076
result =  optimize(loss, [0.0,0.0] )  # optimize f with starting guess

# â•”â•â•¡ 7bd9bb8f-36c5-4ae1-ba20-25732d7fef2e
result.minimizer

# â•”â•â•¡ 10386ce6-82fd-46ea-a44a-6ba14c5b0cd9
md"""
## JuMP.jl: A popular modelling language for Optimization Problems

JuMP = Julia for Mathematical Programming
"""

# â•”â•â•¡ b7d8f11d-91ce-4b3a-87a1-1aa162e198ff
let
	
	n = length(x)
	model  = Model(Ipopt.Optimizer)
	
	@variable(model, b)
	@variable(model, m)

    @objective(model, Min, sum((b+m*x[i]-y[i])^2 for i in 1:n))

	#set_silent(model)
	optimize!(model)
	(b=getvalue(b), m=getvalue(m))
end

# â•”â•â•¡ 5ca85768-a19e-4ddf-89a4-88dca599d7a7
md"""
# Gradients
"""

# â•”â•â•¡ dd39b088-f59f-43fa-bce0-5076398238f9
md"""
The above optimization methods made no explicit mention of derivative or gradient information.  For simple problems, gradients can be hand calculated, but for many real problems this is impractical.
"""

# â•”â•â•¡ 327514f1-8081-4a6c-8be4-8ffd52ed3c46
md"""
## Bells and Whistles
"""

# â•”â•â•¡ 98e00b2d-0802-4160-8e5c-302be5226916
optimize(loss, [0.0,0.0], BFGS(),  autodiff=:forward)

# â•”â•â•¡ ef165ca5-bf4f-465e-8e9a-df1aec2d7caa
optimize(loss, [0.0,0.0], BFGS() )

# â•”â•â•¡ 0305b418-51bb-47bb-98fb-319fc26b94cf
optimize(loss, [0.0,0.0], GradientDescent() )

# â•”â•â•¡ 304a3a6e-c8c3-48d8-a101-313b3aa062f2
optimize(loss, [0.0,0.0], GradientDescent(), autodiff=:forward )

# â•”â•â•¡ Cell order:
# â•Ÿâ”€945c2bf1-d7dc-42c9-93d7-fd754f8fb1d7
# â• â•400ebe26-0dea-4cf2-8744-6c73a45cd33e
# â• â•b8d66df5-f593-40b4-8c46-3b638f9cc3e1
# â•Ÿâ”€dccbd53d-33ed-4d37-9d2c-da76e090d5dd
# â• â•2ed86f33-bced-413c-9a8d-c6e49bfe5afb
# â•Ÿâ”€0e43a6d3-7198-422b-b50c-b9caeaa53074
# â• â•f8c98995-2152-4d45-996a-a0532a233719
# â• â•8a5f1fdc-3cef-4c02-a73f-e5975b57b15a
# â• â•647093eb-a7e3-4175-8091-29c33407e5c9
# â•Ÿâ”€cdc25782-65a8-43c5-8090-c1241b798b1a
# â• â•9ec4dd43-c95a-4f11-b844-fd6ccc93bb68
# â• â•9276b315-27b2-4b01-8fc8-4ebbba58d080
# â• â•d22fd4bd-acfe-4e27-a484-3c2d6138f44e
# â• â•da0d208b-7d30-470a-b180-4cbfa98298e7
# â•Ÿâ”€6cf233a7-9b8b-47aa-a3ad-2440d001af73
# â• â•613c3e5f-bbdd-4cf9-b30f-69e2c42ae0ec
# â• â•4cce580b-0032-419c-b386-e470b084ab96
# â• â•5503b4de-0b53-4223-8ce0-5e014be3f7ab
# â• â•05e512ca-3123-48d9-9c11-5d6e9d90ef95
# â• â•e0b4c2a9-a68b-47af-bf9c-f1a9f0256fd4
# â• â•6d25e38e-c18a-48b3-8b12-b670f5a5180f
# â• â•f291c0cb-51ee-4b30-9e07-e7cf374f809e
# â•Ÿâ”€aa06a447-d6c5-48ee-9864-c1f431fe5e4b
# â•Ÿâ”€d3edfb26-7258-45a3-a88c-60831338df1f
# â• â•372b304a-3f57-4bec-88df-3d51ded57d5c
# â• â•13b9ff38-225d-4ec1-be7f-bf0e0f5b4076
# â• â•7bd9bb8f-36c5-4ae1-ba20-25732d7fef2e
# â• â•10386ce6-82fd-46ea-a44a-6ba14c5b0cd9
# â• â•b7d8f11d-91ce-4b3a-87a1-1aa162e198ff
# â•Ÿâ”€5ca85768-a19e-4ddf-89a4-88dca599d7a7
# â• â•dd39b088-f59f-43fa-bce0-5076398238f9
# â• â•327514f1-8081-4a6c-8be4-8ffd52ed3c46
# â• â•98e00b2d-0802-4160-8e5c-302be5226916
# â• â•ef165ca5-bf4f-465e-8e9a-df1aec2d7caa
# â• â•0305b418-51bb-47bb-98fb-319fc26b94cf
# â• â•304a3a6e-c8c3-48d8-a101-313b3aa062f2
